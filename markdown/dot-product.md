---
title: "Dot Product Explained | The Most Important Vector Operation for ML - Lesson 4"
description: "Master the dot product - the foundation of machine learning predictions. Learn how to calculate scores from features and weights with Python examples."
keywords:
  - dot product
  - inner product
  - linear model
  - prediction score
  - feature weights
  - machine learning basics
slug: /vectors/dot-product/
---

# Lesson 4: Dot Product

**Breadcrumbs:** [Home](/) > [Vectors for Data Science](index.md) > Dot Product

---

## A) What It Means

The dot product takes two vectors and produces a single number. You multiply matching elements, then add everything up.

---

## B) How to Picture It

Formula: a Â· b = aâ‚Ã—bâ‚ + aâ‚‚Ã—bâ‚‚ + aâ‚ƒÃ—bâ‚ƒ + ...

The dot product measures how much two vectors point in the same direction:

```
Same direction:     â†’  â†’      dot product: LARGE POSITIVE
Opposite:           â†’  â†      dot product: NEGATIVE  
Perpendicular:      â†’  â†‘      dot product: ZERO
```

> **ğŸ“· Image Idea (Realistic):** A calculator display showing a multiplication and addition sequence, with customer data on sticky notes nearby
> 
> *Alt text:* Calculator showing dot product computation with customer data
> 
> *Caption:* The dot product: multiply and add to get a prediction score

---

## C) Business Example

**This is the BIG one for data science!**

**Situation:** Calculate a "score" for how likely Alice is to buy.

```
Alice's features: x = [28, 45, 5, 3, 2, 8]
Weights:          w = [0.01, 0.02, 0.5, 0.3, 0.8, 0.4]

Score = w Â· x 
     = (0.01Ã—28) + (0.02Ã—45) + (0.5Ã—5) + (0.3Ã—3) + (0.8Ã—2) + (0.4Ã—8)
     = 0.28 + 0.9 + 2.5 + 0.9 + 1.6 + 3.2
     = 9.38
```

The weights say: "prior purchases (0.8) and visits (0.5) matter a lot; age (0.01) barely matters."

**Higher score â†’ more likely to buy!**

---

## D) Where It Shows Up in ML

**Plain words:** Almost every prediction formula works like this: take the customer's features, multiply each one by how important it is (the weight), and add them up.

**In machine learning, this is the core of "linear models."**

`score = w Â· x + b`

Where:
- `w` = weight vector (importance of each feature)
- `x` = customer's feature vector
- `b` = "bias" or starting point

Linear regression, logistic regression, and neural networks ALL use dot products!

> **ğŸ“Š Image Idea (Infographic):** Visual equation showing features Ã— weights = score, with colored boxes showing each multiplication step combining into a final number
> 
> *Alt text:* Step-by-step dot product calculation infographic
> 
> *Caption:* score = features Â· weights: the heart of ML predictions

---

## E) Worked Numeric Example

```
Customer: x = [30, 50, 7]   (age, income_k, satisfaction)
Weights:  w = [0.1, 0.05, 1.0]

Dot product:
w Â· x = (0.1 Ã— 30) + (0.05 Ã— 50) + (1.0 Ã— 7)
      = 3 + 2.5 + 7
      = 12.5
```

Score = 12.5. Higher numbers = better match!

---

## F) Python (NumPy) Snippet

```python
import numpy as np

# Customer features
x = np.array([30, 50, 7])

# Learned weights
w = np.array([0.1, 0.05, 1.0])

# Dot product - two equivalent ways
score1 = np.dot(w, x)
score2 = np.sum(w * x)  # element-wise multiply, then sum

print("Score (np.dot):", score1)
print("Score (manual):", score2)

# Output:
# Score (np.dot): 12.5
# Score (manual): 12.5
```

### ğŸ§ª Try It Yourself

Run this code in [Google Colab](https://colab.research.google.com/) (free, no installation needed):

1. Go to colab.research.google.com
2. Click "New Notebook"
3. Paste the code above into a cell
4. Press Shift+Enter to run

---

## G) Key Takeaways

> âœ… **What to Remember**
> 
> - Dot product: multiply matching elements, add them up â†’ one number
> - It's the foundation of almost all prediction models
> - score = weights Â· features is how ML models make guesses

---

## H) Common Mistakes

> âŒ **Avoid These Errors**
> 
> - Forgetting vectors must be the same length
> - Mixing up dot product (gives a number) with scalar multiplication (gives a vector)
> - Not realizing order matters conceptually (w Â· x means 'weight the features')

---

## ğŸ“ Practice Questions

**Question 1:** Calculate: [2, 3] Â· [4, 5]

<details>
<summary>Show Answer</summary>

(2Ã—4) + (3Ã—5) = 8 + 15 = 23

</details>

**Question 2:** If weights are [0, 0, 1] and features are [100, 200, 5], what's the score?

<details>
<summary>Show Answer</summary>

Score = 0Ã—100 + 0Ã—200 + 1Ã—5 = 5. Only the third feature matters!

</details>

---

## ğŸ“š Learning Resources

| Type | Resource | Link |
|------|----------|------|
| ğŸ¬ Video | 3Blue1Brown: Dot products and duality | [Link](https://youtube.com/watch?v=LyGKycYT2v0) |
| âœï¸ Practice | Khan Academy: Dot product exercises | [Link](https://khanacademy.org/math/linear-algebra/vectors-and-spaces/dot-cross-products) |
| ğŸ¤– Applied ML | StatQuest: Linear Regression Explained | [Link](https://youtube.com/watch?v=nk2CQITm_eo) |

---

## Navigation

[â† Scalar Multiplication](scalar-multiplication.md) | [Course Home](index.md) | [Cross Product â†’](cross-product.md)

---

*Â© 2026 Your Website Name*
